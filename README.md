# Attend and boost
Combining recurrent neural networks with an attention mechanism and Lorentz transformations, in order to compute attention-weighted Lorentz boosts. This results in a 'learned' rest frame in which meaningful physics quantities can be calculated, and permits interpretation of downstream outputs.

TODO
----
* Implement L1 regularisation to sparsify the track weights that enter the Lorentz boost.
* Incorporate multi-head attention for Seq2Seq-like transformer architecture
